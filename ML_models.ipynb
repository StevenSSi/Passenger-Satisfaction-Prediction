{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Import Files\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report, recall_score\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import tensorflow as tf\n",
    "from numpy import loadtxt\n",
    "import keras\n",
    "import kerastuner as kt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, BatchNormalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pre-processed data\n",
    "df_train = pd.read_csv(\"MIT Hackathon/train_final.csv\")\n",
    "df_validation = pd.read_csv(\"MIT Hackathon/validation_final.csv\")\n",
    "df_test = pd.read_csv(\"MIT Hackathon/test_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14365, 23), (14365,), (14345, 23), (14345,))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split into training and testing x,y\n",
    "y_train = df_train['Overall_Experience']\n",
    "x_train = df_train.drop(columns = \"Overall_Experience\")\n",
    "y_validation = df_validation['Overall_Experience']\n",
    "x_validation = df_validation.drop(columns = \"Overall_Experience\")\n",
    "x_train.shape, y_train.shape, x_validation.shape, y_validation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since DNN function has validation_split option, concat train and validation sets\n",
    "train_final = pd.concat([df_train, df_validation], axis = 0)\n",
    "y_train_final = train_final['Overall_Experience']\n",
    "x_train_final = train_final.drop(columns = \"Overall_Experience\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best Epochs: 25\n",
    "#Batch Size = 32\n",
    "#Best accuracy = 0.9541880\n",
    "\n",
    "h_model2 = keras.Sequential()\n",
    "\n",
    "h_model2.add(keras.layers.Dense(units=384, input_shape=(78,), activation='relu', name='dense_unique'))\n",
    "h_model2.add(BatchNormalization())\n",
    "h_model2.add(keras.layers.Dropout(0.4))\n",
    "h_model2.add(keras.layers.Dense(units=288, input_shape=(78,), activation='relu', name='dense_unique2'))\n",
    "h_model2.add(BatchNormalization())\n",
    "h_model2.add(keras.layers.Dropout(0.4))\n",
    "h_model2.add(keras.layers.Dense(384, activation='relu'))\n",
    "h_model2.add(BatchNormalization())\n",
    "h_model2.add(keras.layers.Dropout(0.3))\n",
    "h_model2.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "hp_learning_rate = 1e-4\n",
    "h_model2.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use GridSearchCV to hyperparameter tune the batch size and epochs\n",
    "callbacks_tuned = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath= \"parameter.keras\",\n",
    "        save_best_only=True,\n",
    "        monitor=\"val_loss\")\n",
    "]\n",
    "\n",
    "params={'batch_size':[100, 20, 50, 25, 32], \n",
    "        'epochs':[25, 50, 100, 200, 300, 400]\n",
    "        }\n",
    "\n",
    "h_model2_gs = GridSearchCV(estimator = h_model2, param_grid = params, cv = 10)\n",
    "\n",
    "h_model2_gs.fit(x= x_train_final, y = y_train_final)\n",
    "\n",
    "prediction_dnn = h_model2_gs.predict(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graphical representation of val_accuracy over time\n",
    "accuracy = history_tuned.history[\"accuracy\"]\n",
    "val_accuracy = history_tuned.history[\"val_accuracy\"]\n",
    "loss = history_tuned.history[\"loss\"]\n",
    "val_loss = history_tuned.history[\"val_loss\"]\n",
    "epochs = range(1, len(accuracy) + 1)\n",
    "plt.plot(epochs, accuracy, \"bo\", label=\"Training accuracy\")\n",
    "plt.plot(epochs, val_accuracy, \"b\", label=\"Validation accuracy\")\n",
    "plt.title(\"Training and validation accuracy\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, \"bo\", label=\"Training loss\")\n",
    "plt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\n",
    "plt.title(\"Training and validation loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Implementation\n",
    "ID_data = pd.read_csv(\"MIT Hackathon/Traveldata_test_(2).csv\")\n",
    "submission_df = pd.DataFrame()\n",
    "submission_df['ID'] = ID_data['ID']\n",
    "submission_df['Overall_Experience'] = prediction_dnn\n",
    "submission_df['Overall_Experience'] = submission_df['Overall_Experience'].round(0).astype(int)\n",
    "submission_df.to_csv(\"submission_data_DNN.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SVM classifier\n",
    "clf = SVC(kernel='linear')\n",
    "\n",
    "# Fit data\n",
    "clf = clf.fit(x_train_final, y_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model prediction\n",
    "predictions = clf.predict(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hp Tuning\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'C': [0.1, 1, 10, 100, 1000], \n",
    "              'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
    "              'kernel': [\"linear\", \"poly\", \"sigmoid\", \"rbf\"]}\n",
    "\n",
    "grid = GridSearchCV(SVC(), param_grid, refit = True, verbose = 3)\n",
    "\n",
    "grid.fit(x_train_final, y_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_predictions = grid.predict(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Implementation\n",
    "ID_data = pd.read_csv(\"Traveldata_test_(2).csv\")\n",
    "submission_df = pd.DataFrame()\n",
    "submission_df['ID'] = ID_data['ID']\n",
    "submission_df['Overall_Experience'] = grid_predictions\n",
    "submission_df['Overall_Experience'] = submission_df['Overall_Experience'].round(0).astype(int)\n",
    "submission_df.to_csv(\"submission_data_SVM.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor,BaggingRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer,mean_squared_error, r2_score, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning on multiple RF parameters\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "\n",
    "max_features = ['auto', 'sqrt']\n",
    "\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "\n",
    "min_samples_split = [2, 5, 10]\n",
    "\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "\n",
    "bootstrap = [True, False]\n",
    "\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor()\n",
    "\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "rf_random.fit(x_train_final, y_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_tuned = rf_random.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check feature importance\n",
    "importances = rf.feature_importances_\n",
    "\n",
    "indices = np.argsort(importances)\n",
    "\n",
    "plt.figure(figsize = (10, 10))\n",
    "\n",
    "plt.title('Feature Importances')\n",
    "\n",
    "plt.barh(range(len(indices)), importances[indices], color = 'violet', align = 'center')\n",
    "\n",
    "plt.yticks(range(len(indices)), [features[i] for i in indices])\n",
    "\n",
    "plt.xlabel('Relative Importance')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Implementation\n",
    "ID_data = pd.read_csv(\"Traveldata_test_(2).csv\")\n",
    "submission_df = pd.DataFrame()\n",
    "submission_df['ID'] = ID_data['ID']\n",
    "submission_df['Overall_Experience'] = prediction_tuned\n",
    "submission_df['Overall_Experience'] = submission_df['Overall_Experience'].round(0).astype(int)\n",
    "submission_df.to_csv(\"submission_data_RF.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_RF = pd.read_csv(\"submission_data_RF.csv\")\n",
    "submission_SVM = pd.read_csv(\"submission_data_SVM.csv\")\n",
    "submission_DNN = pd.read_csv(\"submission_data_DNN.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_RF.rename(columns={'Overall_Experience': 'RF_Model'}, inplace=True)\n",
    "submission_SVM.rename(columns={'Overall_Experience': 'SVM_Model'}, inplace=True)\n",
    "submission_DNN.rename(columns={'Overall_Experience': 'DNN_Model'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_RF.drop(columns = \"ID\", inplace = True)\n",
    "submission_SVM.drop(columns = \"ID\", inplace = True)\n",
    "submission_DNN.drop(columns = \"ID\", inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_table = pd.concat([submission_RF, submission_SVM, submission_DNN], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_table['Final_Pred'] = comparison_table.iloc[:, -5:].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_table['Overall_Experience'] = comparison_table['Final_Pred'].apply(lambda x: 1 if x >= 2 else 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_table.drop(columns = [\"RF_Model\",\"SVM_Model\",\"DNN_Model\"], inplace = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
